# Cloud Architecture Documentation

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLIENT APPLICATION                        â”‚
â”‚                    (Your Frontend/Form)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ HTTPS POST
                             â”‚ (company data + files)
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PAPERSPACE DEPLOYMENT                         â”‚
â”‚                   (webhook_server_cloud.py)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ 1. Receive webhook request                             â”‚     â”‚
â”‚  â”‚ 2. Save to /outputs/{company_slug}/                    â”‚     â”‚
â”‚  â”‚ 3. Download Google Drive files (if provided)           â”‚     â”‚
â”‚  â”‚ 4. Trigger Paperspace Workflow                         â”‚     â”‚
â”‚  â”‚ 5. Return confirmation code                            â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ Gradient API
                             â”‚ (trigger workflow)
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PAPERSPACE WORKFLOW                            â”‚
â”‚                  (pipeline-workflow.yaml)                        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   JOB 1:     â”‚  â”‚   JOB 2:     â”‚  â”‚   JOB 3:     â”‚         â”‚
â”‚  â”‚    Setup     â”‚â”€â–¶â”‚Vector Store  â”‚â”€â–¶â”‚   Pipeline   â”‚         â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚         â”‚
â”‚  â”‚ â€¢ Clone repo â”‚  â”‚ â€¢ Preprocess â”‚  â”‚ â€¢ Part A     â”‚         â”‚
â”‚  â”‚ â€¢ Download   â”‚  â”‚ â€¢ Upload to  â”‚  â”‚ â€¢ Part B     â”‚         â”‚
â”‚  â”‚   Drive      â”‚  â”‚   OpenAI     â”‚  â”‚ â€¢ Consolidateâ”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                              â”‚                  â”‚
â”‚                                              â–¼                  â”‚
â”‚                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                                       â”‚   JOB 4:     â”‚          â”‚
â”‚                                       â”‚   Upload     â”‚          â”‚
â”‚                                       â”‚              â”‚          â”‚
â”‚                                       â”‚ â€¢ Send to    â”‚          â”‚
â”‚                                       â”‚   Drive      â”‚          â”‚
â”‚                                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                                                â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  GOOGLE DRIVE       â”‚
                                    â”‚  Final Report ğŸ“„    â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Component Details

### 1. Webhook Server (Always-On Deployment)

**File:** `paperspace/webhook_server_cloud.py`

**Instance Type:** C5 (CPU, 2 vCPU, 8GB RAM)

**Responsibilities:**
- Receive POST requests from clients
- Parse company data and files
- Save submissions to persistent storage
- Download Google Drive files
- Trigger pipeline workflows
- Return immediate confirmation

**Endpoints:**
- `GET /health` - Health check
- `GET /test` - Test endpoint
- `POST /` - Main webhook endpoint
- `GET /storage/{company_slug}` - Storage stats

**Storage:**
```
/outputs/
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ webhook_server_TIMESTAMP.log
â””â”€â”€ {company_slug}/
    â”œâ”€â”€ debug/
    â”‚   â””â”€â”€ submission_TIMESTAMP.json
    â””â”€â”€ data/
        â”œâ”€â”€ uploaded_file1.pdf
        â””â”€â”€ downloaded_from_drive.txt
```

### 2. Pipeline Workflow (On-Demand Jobs)

**File:** `paperspace/workflows/pipeline-workflow.yaml`

#### Job 1: Setup (C5 Instance)

**Duration:** ~5 minutes  
**Cost:** ~$0.01

```
Responsibilities:
â”œâ”€â”€ Clone GitHub repository
â”œâ”€â”€ Download Google Drive files
â”œâ”€â”€ Create company_info.json
â””â”€â”€ Output: company-data volume
```

#### Job 2: Vector Store (C7 Instance)

**Duration:** ~3 minutes  
**Cost:** ~$0.01

```
Responsibilities:
â”œâ”€â”€ Load company data
â”œâ”€â”€ Convert CSV â†’ JSON
â”œâ”€â”€ Upload to OpenAI vector store
â””â”€â”€ Output: vector-id volume
```

#### Job 3: Pipeline (P4000 GPU Instance)

**Duration:** ~45 minutes  
**Cost:** ~$0.38

```
Responsibilities:
â”œâ”€â”€ Install AI dependencies
â”œâ”€â”€ Load vector store ID
â”œâ”€â”€ Run pipeline_cloud.py
â”‚   â”œâ”€â”€ Part A: Initial analysis
â”‚   â”œâ”€â”€ Quotes extraction
â”‚   â”œâ”€â”€ Part B: Use cases generation
â”‚   â””â”€â”€ Consolidation: Final report
â””â”€â”€ Output: final-reports dataset
```

#### Job 4: Upload (C5 Instance)

**Duration:** ~2 minutes  
**Cost:** ~$0.01

```
Responsibilities:
â”œâ”€â”€ Find latest final report
â”œâ”€â”€ Upload to Google Drive
â””â”€â”€ Complete workflow
```

## ğŸ”„ Data Flow

### Input Flow (Client â†’ Pipeline)

```
Client Submission
    â”‚
    â”œâ”€â–º company_name â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Used for directory naming
    â”œâ”€â–º company_description â”€â”€â”€â”€â”€â–º Passed to pipeline
    â”œâ”€â–º use_cases_count â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Controls generation
    â”œâ”€â–º readiness_score â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Context for analysis
    â”œâ”€â–º google_drive_link â”€â”€â”€â”€â”€â”€â”€â–º Downloaded files
    â””â”€â–º uploaded_files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Saved to data/

           â†“ (stored in)

/outputs/{company_slug}/
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ {uploaded files}
    â”‚   â””â”€â”€ {downloaded from Drive}
    â””â”€â”€ debug/
        â””â”€â”€ submission_{timestamp}.json

           â†“ (processed by)

Pipeline Jobs
    â”œâ”€â”€ Setup: Prepares environment
    â”œâ”€â”€ Vector Store: Creates embeddings
    â”œâ”€â”€ Pipeline: Generates insights
    â””â”€â”€ Upload: Delivers final report

           â†“ (results in)

/outputs/{company_slug}/final/
    â””â”€â”€ FINAL_REPORT_{timestamp}.md

           â†“ (uploaded to)

Google Drive
    â””â”€â”€ Final Reports Folder/
        â””â”€â”€ FINAL_REPORT_{company_slug}_{timestamp}.md
```

### Log Flow (Pipeline â†’ Monitoring)

```
Python Code
    â”‚
    â””â”€â–º logger.info("message", key=value)
           â”‚
           â”œâ”€â–º JSON Formatter
           â”‚      â”‚
           â”‚      â””â”€â–º {"timestamp":"...","level":"INFO","message":"...","key":"value"}
           â”‚
           â”œâ”€â–º stdout (captured by Paperspace)
           â”‚      â”‚
           â”‚      â””â”€â–º Paperspace Logs UI
           â”‚
           â””â”€â–º File Handler
                  â”‚
                  â””â”€â–º /outputs/logs/pipeline_TIMESTAMP.log
                         â”‚
                         â””â”€â–º Persistent storage
```

## ğŸ—„ï¸ Storage Architecture

### Persistent Volumes

```
Volume: webhook-storage (10GB)
Mount: /outputs
Lifecycle: Persistent across deployments

/outputs/
â”œâ”€â”€ logs/                           # All service logs
â”‚   â”œâ”€â”€ webhook_server_*.log
â”‚   â””â”€â”€ pipeline_*.log
â”‚
â”œâ”€â”€ company_a/                      # Isolated per company
â”‚   â”œâ”€â”€ debug/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ vector_ids/
â”‚   â”œâ”€â”€ part_a/
â”‚   â”œâ”€â”€ part_b/
â”‚   â””â”€â”€ final/
â”‚
â”œâ”€â”€ company_b/
â”‚   â””â”€â”€ [same structure]
â”‚
â””â”€â”€ company_c/
    â””â”€â”€ [same structure]
```

### Ephemeral Storage

```
/tmp/                               # Temporary files
â”œâ”€â”€ drive_download/                 # Downloaded Drive files (before move)
â””â”€â”€ preprocessing/                  # Temporary processing files

Note: /tmp is cleared after job completion
```

## ğŸ” Security Architecture

### Secrets Management

```
GitHub Secrets                      Paperspace Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PAPERSPACE_API_KEY    â”€â”€â”€â”€â”€â”€â”€â”€â–º   Used by GitHub Actions
PAPERSPACE_PROJECT_ID â”€â”€â”€â”€â”€â”€â”€â”€â–º   Used by GitHub Actions
OPENAI_API_KEY        â”€â”€â”€â”€â”€â”€â”€â”€â–º   Injected as ENV var
GDRIVE_CREDENTIALS    â”€â”€â”€â”€â”€â”€â”€â”€â–º   Mounted to /app/secrets/

                                   Container
                                   â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                   ENV: OPENAI_API_KEY
                                   FILE: /app/secrets/google_drive_credentials.json
```

### Network Security

```
Internet
    â”‚
    â””â”€â–º HTTPS only (Paperspace enforces TLS)
           â”‚
           â””â”€â–º Webhook Server
                  â”‚
                  â”œâ”€â–º CORS: Configured origins only
                  â”œâ”€â–º Rate limiting: TBD
                  â””â”€â–º Auth: Optional OPENAI_WEBHOOK_SECRET
```

## ğŸ“Š Monitoring & Observability

### Metrics Collected

```python
# Logged automatically by CloudLogger

logger.metric("upload_size", 1024, "KB")
# â†’ {"metric_name":"upload_size","metric_value":1024,"metric_unit":"KB"}

logger.metric("use_cases_count", 7)
# â†’ {"metric_name":"use_cases_count","metric_value":7}

logger.metric("pipeline_duration", 3600, "seconds")
# â†’ {"metric_name":"pipeline_duration","metric_value":3600,"metric_unit":"seconds"}
```

### Log Aggregation

```
Source: Python code
    â†“
Stdout: JSON formatted
    â†“
Paperspace Logs: Real-time aggregation
    â†“
Retention: 30 days (Paperspace default)
    â†“
Export: Available via API or UI
```

## ğŸ” Failure Handling

### Webhook Server Failures

```
Request arrives
    â†“
Try: Save submission
    â”œâ”€â–º Success: Continue
    â””â”€â–º Failure: Log error, save to /outputs/error_*.json
           â†“
Try: Trigger workflow
    â”œâ”€â–º Success: Return confirmation code
    â””â”€â–º Failure: Log error, return 500 (client can retry)
           â†“
Note: Submission is ALWAYS saved, even if workflow fails
```

### Pipeline Failures

```
Pipeline Job
    â†“
Try: Each step (with retries built-in)
    â”œâ”€â–º Success: Continue to next step
    â””â”€â–º Failure: Log detailed error
           â†“
           â”œâ”€â–º Transient error (e.g., API timeout)
           â”‚      â”œâ”€â–º Retry 2 more times
           â”‚      â””â”€â–º If all fail: Mark job failed
           â”‚
           â””â”€â–º Permanent error (e.g., missing file)
                  â””â”€â–º Mark job failed immediately

Result: Workflow run marked as failed in Paperspace
    â†“
Notification: Email/Slack (if configured)
    â†“
Manual intervention: Review logs, fix issue, re-trigger
```

## ğŸš€ Scaling Strategy

### Horizontal Scaling (Multiple Companies)

```
Webhook Server (1 instance)
    â”‚
    â”œâ”€â–º Workflow Run 1 (Company A) â”€â”€â–º P4000 instance
    â”œâ”€â–º Workflow Run 2 (Company B) â”€â”€â–º P4000 instance
    â”œâ”€â–º Workflow Run 3 (Company C) â”€â”€â–º P4000 instance
    â””â”€â–º Workflow Run N (Company N) â”€â”€â–º P4000 instance

Note: Unlimited parallel workflows, each isolated
```

### Vertical Scaling (Larger Companies)

```yaml
# In pipeline-workflow.yaml

jobs:
  pipeline:
    resources:
      # Small company (< 100 docs)
      instance-type: P4000
      
      # Medium company (100-500 docs)
      instance-type: P5000
      
      # Large company (500+ docs)
      instance-type: V100
```

## ğŸ’° Cost Optimization

### Instance Selection Matrix

```
Task Type          CPU Need   GPU Need   Instance    Cost/hr
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€
File I/O           Low        None       C5          $0.09
Data processing    Medium     None       C7          $0.14
AI inference       High       Yes        P4000       $0.51
Large models       High       Yes (big)  V100        $2.30
```

### Cost Per Pipeline Run

```
Setup (C5, 5 min)          = $0.01
Vector Store (C7, 3 min)   = $0.01
Pipeline (P4000, 45 min)   = $0.38
Upload (C5, 2 min)         = $0.01
                           â”€â”€â”€â”€â”€
TOTAL                      = $0.41 per company

Monthly (100 companies)    = $41.00
Webhook (always-on C5)     = $6.48
                           â”€â”€â”€â”€â”€
TOTAL MONTHLY              = $47.48
```

## ğŸ”® Future Architecture Enhancements

### Phase 2: Caching

```
Vector Store Upload
    â†“
Check: Cache for similar companies
    â”œâ”€â–º Cache hit: Reuse vector store
    â”‚      â””â”€â–º Save: ~3 min, ~$0.01
    â””â”€â–º Cache miss: Create new
           â””â”€â–º Cost: Original
```

### Phase 3: Queue System

```
Webhook Server
    â†“
Save to Queue (Redis/SQS)
    â†“
Queue Worker (monitors queue)
    â”œâ”€â–º Available slot: Trigger workflow
    â””â”€â–º Queue full: Wait and retry
           â””â”€â–º Prevents overwhelming Paperspace
```

### Phase 4: Multi-Region

```
Client Location
    â”œâ”€â–º US East â”€â”€â–º Paperspace US East
    â”œâ”€â–º EU â”€â”€â”€â”€â”€â”€â”€â–º Paperspace EU
    â””â”€â–º APAC â”€â”€â”€â”€â”€â–º Paperspace APAC

Benefits:
â”œâ”€â–º Lower latency
â”œâ”€â–º Compliance (data residency)
â””â”€â–º High availability
```

## ğŸ“š Reference

### Directory Structure Map

```
paperspace_pipeline/
â”œâ”€â”€ paperspace/                     # Cloud-specific code
â”‚   â”œâ”€â”€ webhook_server_cloud.py    # Webhook server
â”‚   â”œâ”€â”€ pipeline_cloud.py          # Pipeline runner
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ cloud_logging.py       # Logging
â”‚   â”‚   â””â”€â”€ cloud_storage.py       # Storage
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ preprocess_data.py     # Preprocessing
â”‚   â”‚   â””â”€â”€ download_gdrive.py     # Drive downloads
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ webhook-deployment.yaml # Deployment spec
â”‚       â””â”€â”€ pipeline-workflow.yaml  # Workflow spec
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ deploy-paperspace.yml      # CI/CD
â”‚
â”œâ”€â”€ webhook_server.py              # Original (local)
â”œâ”€â”€ pipeline_minimal.py            # Original (local)
â”œâ”€â”€ part_a/                        # Part A code
â”œâ”€â”€ part_b/                        # Part B code
â””â”€â”€ final_consolidation/           # Consolidation code
```

### API Reference

**Webhook Server Endpoints:**

```
GET /health
Response: {"status":"healthy","service":"webhook_server_cloud",...}

POST /
Body: {"company_name":"Acme","use_cases_count":7,...}
Response: {"status":"success","confirmation_code":"CONF_...",.. }

GET /storage/{company_slug}
Response: {"company":"acme","total_size_bytes":1024,...}
```

**Gradient API (used internally):**

```python
from gradient import WorkflowsClient

client = WorkflowsClient(api_key=API_KEY)
run = client.run_workflow(
    workflow_id=WORKFLOW_ID,
    inputs={"company_name": "Acme", ...}
)
```

---

**Last Updated:** October 23, 2025  
**Version:** 1.0  
**Maintained By:** AI Pipeline Team

